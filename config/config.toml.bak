              # Controls randomness

[llm]
api_type = "ollama"
model= "Musai:latest"
#model = "ALIENTELLIGENCE/deepseekcoder16kcontextv2:latest"
#model = "qwen2.5-coder:7b"
base_url = "http://localhost:11434/v1"
api_key = "ollama"  # Ollama doesn't require this, but some systems expect a value
max_tokens = 4096
temperature = 0.2
# Controls randomness for vision model

[llm.vision] #OLLAMA VISION:
api_type = 'ollama'
model = "llama3.2-vision"
base_url = "http://localhost:11434/v1"
api_key = "ollama"
max_tokens = 4096
temperature = 0.0

# Optional configuration for specific browser configuration
#[browser]
# Whether to run browser in headless mode (default: false)
#headless = false
# Disable browser security features (default: true)
#disable_security = true
# Extra arguments to pass to the browser
#extra_chromium_args = []
# Path to a Chrome instance to use to connect to your normal browser
# e.g. '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
#chrome_instance_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
# Connect to a browser instance via WebSocket
#wss_url = ""
# Connect to a browser instance via CDP
#cdp_url = ""

[browser]
headless = false
disable_security = true
extra_chromium_args = [
    "--disable-blink-features=AutomationControlled",
    "--disable-features=VizDisplayCompositor",
    "--exclude-switches=enable-automation",
    "--disable-extensions-file-access-check",
    "--disable-extensions-http-throttling"
]
chrome_instance_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"

# âœ… Add this to use your running Chrome's debug port
cdp_url = "http://localhost:9222"
#wss_url = ""

# Optional configuration, Proxy settings for the browser
# [browser.proxy]
# server = "http://proxy-server:port"
# username = "proxy-username"
# password = "proxy-password"

# Optional configuration, Search settings.
[search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo" or "Bing".
engine = "Google"
# Fallback engine order. Default is ["DuckDuckGo", "Baidu", "Bing"] - will try in this order after primary engine fails.
fallback_engines = ["DuckDuckGo", "Baidu", "Bing"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
max_retries = 3
# Language code for search results. Options: "en" (English), "zh" (Chinese), etc.
lang = "en"
# Country code for search results. Options: "us" (United States), "cn" (China), etc.
country = "ca"


## Sandbox configuration
[sandbox]
use_sandbox = false
image = "python:3.12-slim"
work_dir = "/workspace"
memory_limit = "1g"  # 512m
cpu_limit = 2.0
timeout = 300
network_enabled = true

# MCP (Model Context Protocol) configuration
[mcp]
server_reference = "app.mcp.server" # default server module reference

# Optional Runflow configuration
# Your can add additional agents into run-flow workflow to solve different-type tasks.
[runflow]
use_data_analysis_agent = false     # The Data Analysi Agent to solve various data analysis tasks
